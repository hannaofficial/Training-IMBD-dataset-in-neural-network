{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cb4165-834d-4286-bffb-60ac48f502fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9ea414-d6ae-45c3-a019-46214bdd8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('IMDB Dataset.csv')\n",
    "raw_reviews = file['review'].to_list()\n",
    "raw_labels = file['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e338018f-aebf-4872-aa95-636476b3c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(lambda x:set(x.split(\" \")), raw_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f8f5a8-626f-40b4-a34e-7dc2ad9f35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    i=0\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e853c1-80d7-4ea6-9eed-1d9d2be32dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74368835-72a3-486f-9f3b-7e018926dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb89edb8-37d7-4b27-a2fe-d3a79fe88247",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd949072-3576-4761-a3fe-9de5b86d5160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size, window, negative = (50,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af78b110-027d-47c4-8c47-8732b1eaed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5)*0.2\n",
    "weights_1_2 = np.random.rand(len(vocab), hidden_size)*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef32e3b6-c6c5-4265-b98a-f6e315c10e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(weights_0_1[[2,5000]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80aa790d-c4ba-4fa2-bcb9-72e5f16dfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_target = np.zeros(negative + 1)\n",
    "layer_2_target[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85bab0e9-a723-4c12-8eb7-d3729cfe83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(target=\"beautiful\"):\n",
    "    target_index = word2index[target]\n",
    "\n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = raw_difference**2\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717c4f28-730c-44a1-81ef-5a2da92975bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304455"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaffb8a8-3230-42b2-8769-71bffa68212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [3, 4], [2, 3], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "print([[2,3],[3,4]]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e2783c-a115-479b-bee3-09b0c3a03792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.9955 [('terrible', -0.0), ('IS', -3.239742908875227), ('Why', -3.3158155914355394), ('Mel', -3.3231003596491866), ('small', -3.3235185250897312), ('fantasy', -3.3445382818994247), ('production,', -3.3815141050188418), (\"would've\", -3.4646347376558895), ('Friday', -3.4909255663519727), ('fast', -3.537610492020584)])]956)]208420493420245)]\n",
      "Answer: [('terrible', -0.0), ('IS', -3.243674572102079), ('fantasy', -3.2583295560895733), ('Mel', -3.345444530088766), (\"would've\", -3.3574828743416583), ('small', -3.360578787748396), ('Why', -3.3653191772127298), ('fast', -3.3897972697187044), ('production,', -3.4108757562782843), ('Friday', -3.420316421565572)]\n"
     ]
    }
   ],
   "source": [
    "for rev_i, review in enumerate(input_dataset*iterations):\n",
    "    for target_i in range(len(review)):\n",
    "        target_samples = [review[target_i]] + list(concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "        \n",
    "        left_context = review[max(0,target_i - window): target_i]\n",
    "        \n",
    "        right_context = review[target_i+ 1: min(len(review), target_i+window)]\n",
    "\n",
    "        layer_1 = np.mean(weights_0_1[left_context+right_context], axis=0)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "\n",
    "        weights_0_1[left_context+right_context] -= layer_1_delta*alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta, layer_1)*alpha\n",
    "    if (rev_i%500 == 0):\n",
    "         sys.stdout.write('\\rProgress:  '+ str(rev_i/float(len(input_dataset)*iterations)) + ' ' + str(similar('terrible')))\n",
    "         sys.stdout.write('\\rProgress: '+ str(rev_i/float(len(input_dataset)*iterations)))\n",
    "print(\"\")        \n",
    "print(f\"Answer: {similar('terrible')}\")\n",
    "       \n",
    "   \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e7341f-9af6-4e93-93f9-645fd9bde0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.sum(weights_0_1**2, axis=1)\n",
    "norms.resize(norms.shape[0],1)\n",
    "normed_weights = weights_0_1*norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6262eae-9ea5-4d89-a965-6a79d147e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sent_vect(words):\n",
    "    indices = list(map(lambda x:word2index[x], filter(lambda x:x in word2index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e3bf7ca-8086-449e-b45b-58b2f27e7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "review2vectors = list()\n",
    "for review in tokens:\n",
    "    review2vectors.append(make_sent_vect(review))\n",
    "review2vectors = np.array(review2vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b34cfa8-a3c0-4e82-838c-f04e52e77430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Counter()\n",
    "    for i,val in enumerate(review2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    most_similar = list()\n",
    "    for idx,score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0:40])\n",
    "    return most_similar    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d38b1d5-59b1-43c5-9037-770a9d1e42ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My favorite movie. What a great story th',\n",
       " 'As with most Rosalind Russell movies, th',\n",
       " 'A great film in its genre, the direction']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews(['boring','worst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67a5c734-fdaf-41fd-bd9b-9e23a51d4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3ef6967-781d-4818-bccc-d990e089abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5233e722-8c67-4fa6-a31c-a149082d1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").replace(\".\",\"\").split(\" \")[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6956302a-33c7-4685-a5fb-ee224a1b9a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom'], ['john', 'went', 'to', 'the', 'hallway'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e6bf666-7126-40f6-b1e0-b6774eee1d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6f7cdff-5b50-45fb-906e-74feae3835dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x/e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df322774-497c-4819-8bf6-869da6bc23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17170836-a5b6-4164-8639-a5819667b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5)*0.1\n",
    "\n",
    "recurrent = np.eye(embed_size)\n",
    "\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5)*0.1\n",
    "\n",
    "one_hot = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84caadb5-10f6-4f68-a7ec-ed160110a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "\n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cf11696-7397-428b-87fb-3982193875b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "    layers, loss = predict(sent)\n",
    "\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx - 1]\n",
    "\n",
    "        if(layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            if(layer_idx == len(layers) - 1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose( ))\n",
    "        else:  \n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n",
    "\n",
    "    start -= layers[0]['hidden_delta']*alpha/float(len(sent))\n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta'])*alpha/float(len(sent))\n",
    "\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta']*alpha/float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta'])*alpha/float(len(sent))\n",
    "\n",
    "    if(iter % 1000 ==0):\n",
    "        print(\"Perplexity: \"+ str(np.exp(loss/len(sent))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
